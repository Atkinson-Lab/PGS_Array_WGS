{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d2c0aa0d-45f7-4051-bcc7-06f5892d805b\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "'use strict';\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    function drop(id) {\n",
       "      const view = Bokeh.index.get_by_id(id)\n",
       "      if (view != null) {\n",
       "        view.model.document.clear()\n",
       "        Bokeh.index.delete(view)\n",
       "      }\n",
       "    }\n",
       "\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null) {\n",
       "      drop(id)\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim()\n",
       "            drop(id)\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded(error = null) {\n",
       "    const el = document.getElementById(\"d2c0aa0d-45f7-4051-bcc7-06f5892d805b\");\n",
       "    if (el != null) {\n",
       "      const html = (() => {\n",
       "        if (typeof root.Bokeh === \"undefined\") {\n",
       "          if (error == null) {\n",
       "            return \"BokehJS is loading ...\";\n",
       "          } else {\n",
       "            return \"BokehJS failed to load.\";\n",
       "          }\n",
       "        } else {\n",
       "          const prefix = `BokehJS ${root.Bokeh.version}`;\n",
       "          if (error == null) {\n",
       "            return `${prefix} successfully loaded.`;\n",
       "          } else {\n",
       "            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n",
       "          }\n",
       "        }\n",
       "      })();\n",
       "      el.innerHTML = html;\n",
       "\n",
       "      if (error != null) {\n",
       "        const wrapper = document.createElement(\"div\");\n",
       "        wrapper.style.overflow = \"auto\";\n",
       "        wrapper.style.height = \"5em\";\n",
       "        wrapper.style.resize = \"vertical\";\n",
       "        const content = document.createElement(\"div\");\n",
       "        content.style.fontFamily = \"monospace\";\n",
       "        content.style.whiteSpace = \"pre-wrap\";\n",
       "        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n",
       "        content.textContent = error.stack ?? error.toString();\n",
       "        wrapper.append(content);\n",
       "        el.append(wrapper);\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(() => display_loaded(error), 100);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "\n",
       "  const inline_js = [    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "function(Bokeh) {\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      try {\n",
       "            for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "\n",
       "      } catch (error) {display_loaded(error);throw error;\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"d2c0aa0d-45f7-4051-bcc7-06f5892d805b\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"d2c0aa0d-45f7-4051-bcc7-06f5892d805b\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.1.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d2c0aa0d-45f7-4051-bcc7-06f5892d805b\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## import packages\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "import hail as hl\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.getenv('WORKSPACE_CDR')\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array PGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-c.c.terra-vpc-sc-fd39b54c.internal:42113\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20241223-2314-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "# read matrix table \n",
    "mt_array = hl.read_matrix_table(f\"{bucket}/ArrayData/Array_GT_QCed.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 23:15:27.296 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 23:15:31.439 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, BMI: float64, Height: float64, DBP: float64, SBP: float64, HbA1c: float64, leukocyte: float64, Lymphocyte: float64, RBC: float64, Neutrophil: float64, Hemoglobin_concentration: float64, hematocrit_percentage: float64, Eosinophil: float64, Platelet: float64, Monocyte: float64, MCV: float64, MCH: float64, Basophil: float64, MCHC: float64, HDL: float64, LDL: float64, TC: float64, TG: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  float64: 38\n",
      "  int32: 24\n",
      "  str: 1\n",
      "2024-12-23 23:15:32.490 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 23:15:35.586 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, Asthma: float64, Breast_Cancer: float64, CHD: float64, Colorectal_Cancer: int32, Prostate_Cancer: int32, T2D: float64, T1D: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  int32: 26\n",
      "  float64: 21\n",
      "  str: 1\n"
     ]
    }
   ],
   "source": [
    "## read Sample_quant \n",
    "Sample_quant = hl.import_table(f\"{bucket}/Pheno/quant_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_quant = Sample_quant.key_by(\"person_id\")\n",
    "mt_array_quant = mt_array.semi_join_cols(Sample_quant)\n",
    "mt_array_quant = mt_array_quant.annotate_cols(**Sample_quant[mt_array_quant.s])\n",
    "\n",
    "\n",
    "#### read Sample_binary\n",
    "Sample_binary = hl.import_table(f\"{bucket}/Pheno/binary_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_binary = Sample_binary.key_by(\"person_id\")\n",
    "mt_array_binary = mt_array.semi_join_cols(Sample_binary)\n",
    "mt_array_binary = mt_array_binary.annotate_cols(**Sample_binary[mt_array_binary.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Height\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:01:47.704 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 19:01:50.213 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: DBP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:01:51.061 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 19:01:53.567 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: HDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:01:54.388 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 19:01:56.899 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: TC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:01:57.629 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 19:02:00.152 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: RBC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:02:00.915 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 19:02:03.468 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: leukocyte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 19:02:04.212 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 19:02:06.873 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    }
   ],
   "source": [
    "# List of quant phenotypes\n",
    "phenotypes = [\"Height\", \"DBP\", \"HDL\", \"TC\", \"RBC\", \"leukocyte\"]\n",
    "\n",
    "# Loop through each phenotype and annotate it to the Matrix Table\n",
    "for phenotype in phenotypes:\n",
    "    print(f\"Processing phenotype: {phenotype}\")\n",
    "    \n",
    "    # Import the phenotype table\n",
    "    phenotype_table = hl.import_table(\n",
    "        f'{bucket}/PRScs/hg38/Array_{phenotype}_pst_eff_a1_b0.5_phiauto.txt',\n",
    "        impute=True,\n",
    "        types = {\"locus\": hl.tlocus(\"GRCh38\"),\n",
    "                \"alleles\":hl.tarray(hl.tstr)}\n",
    "    )\n",
    "    \n",
    "    phenotype_table = phenotype_table.key_by('locus', 'alleles')\n",
    "    \n",
    "    # Annotate rows in the Matrix Table with the phenotype data\n",
    "    mt_array_quant = mt_array_quant.annotate_rows(\n",
    "        **{f\"{phenotype}_PRScs_result\": phenotype_table[mt_array_quant.locus, mt_array_quant.alleles]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: T2D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 23:15:36.519 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 23:15:39.279 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Asthma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 23:15:40.016 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 23:15:42.623 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Breast_Cancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 23:15:43.329 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 23:15:46.003 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Colorectal_Cancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 23:15:46.778 Hail: INFO: Reading table to impute column types\n",
      "2024-12-23 23:15:49.299 Hail: INFO: Finished type imputation        (0 + 1) / 1]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    }
   ],
   "source": [
    "# List of binary phenotypes\n",
    "phenotypes = [\"T2D\", \"Asthma\", \"Breast_Cancer\", \"Colorectal_Cancer\"]\n",
    "\n",
    "# Loop through each phenotype and annotate it to the Matrix Table\n",
    "for phenotype in phenotypes:\n",
    "    print(f\"Processing phenotype: {phenotype}\")\n",
    "    \n",
    "    # Import the phenotype table\n",
    "    phenotype_table = hl.import_table(\n",
    "        f'{bucket}/PRScs/hg38/Array_{phenotype}_pst_eff_a1_b0.5_phiauto.txt',\n",
    "        impute=True,\n",
    "        types = {\"locus\": hl.tlocus(\"GRCh38\"),\n",
    "                \"alleles\":hl.tarray(hl.tstr)}\n",
    "    )\n",
    "    \n",
    "    phenotype_table = phenotype_table.key_by('locus', 'alleles')\n",
    "    \n",
    "    # Annotate rows in the Matrix Table with the phenotype data\n",
    "    mt_array_binary = mt_array_binary.annotate_rows(\n",
    "        **{f\"{phenotype}_PRScs_result\": phenotype_table[mt_array_binary.locus, mt_array_binary.alleles]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PGS\n",
    "phenotypes = [\"Height\", \"DBP\", \"HDL\", \"TC\", \"RBC\", \"leukocyte\"]\n",
    "\n",
    "# Annotate PGS for all phenotypes\n",
    "mt_array_quant = mt_array_quant.annotate_cols(**{\n",
    "    f\"{phenotype}_pgs\": hl.struct(\n",
    "        pgs=hl.agg.sum(\n",
    "            mt_array_quant[f\"{phenotype}_PRScs_result\"][\"BETA_posterior\"] * mt_array_quant.GT\n",
    "        )\n",
    "    )\n",
    "    for phenotype in phenotypes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PGS\n",
    "phenotypes = [\"T2D\", \"Asthma\", \"Breast_Cancer\", \"Colorectal_Cancer\"]\n",
    "\n",
    "# Annotate PGS for all phenotypes\n",
    "mt_array_binary = mt_array_binary.annotate_cols(**{\n",
    "    f\"{phenotype}_pgs\": hl.struct(\n",
    "        pgs=hl.agg.sum(\n",
    "            mt_array_binary[f\"{phenotype}_PRScs_result\"][\"BETA_posterior\"] * mt_array_binary.GT\n",
    "        )\n",
    "    )\n",
    "    for phenotype in phenotypes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Scores(mt, pheno, mt_type, method):\n",
    "    export_filename = f\"{bucket}/Scores/\" + mt_type + \"/\" + pheno + \"_\" + method + \".bgz\"\n",
    "    sample_info = mt.cols().select(\n",
    "        \"Age\", 'is_sex_Male', 'is_sex_Female', \n",
    "        \"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \n",
    "        \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\", \"PC14\", \"PC15\", \"PC16\", \n",
    "        pheno + \"_pgs\", pheno)\n",
    "    sample_info = sample_info.annotate(**sample_info[pheno + \"_pgs\"])\n",
    "    sample_info = sample_info.drop(pheno + \"_pgs\")\n",
    "    sample_info.export(export_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 18:20:56.072 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n",
      "2024-12-23 18:21:08.268 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:21:15.819 Hail: INFO: wrote table with 685739 rows in 1 partition to /tmp/__iruid_29119-PDhPgO3wBJCGdm5Zlq2NWo\n",
      "2024-12-23 18:25:05.672 Hail: INFO: Coerced sorted dataset======> (39 + 1) / 40]\n",
      "2024-12-23 18:25:07.630 Hail: INFO: merging 41 files totalling 6.7M...+ 8) / 40]\n",
      "2024-12-23 18:25:08.061 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/DBP_PRScs_hg38.bgz\n",
      "  merge time: 430.685ms\n",
      "2024-12-23 18:25:18.821 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:25:23.381 Hail: INFO: wrote table with 685737 rows in 1 partition to /tmp/__iruid_36008-rP9YPm8WyCnpoM9dmtqb8F\n",
      "2024-12-23 18:29:08.289 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:29:09.310 Hail: INFO: merging 33 files totalling 6.6M...+ 8) / 32]\n",
      "2024-12-23 18:29:09.773 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/HDL_PRScs_hg38.bgz\n",
      "  merge time: 462.583ms\n",
      "2024-12-23 18:29:19.942 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:29:24.834 Hail: INFO: wrote table with 685742 rows in 1 partition to /tmp/__iruid_42897-F4T0QnNkuVYE8t6AABcAWt\n",
      "2024-12-23 18:33:07.935 Hail: INFO: Coerced sorted dataset        (32 + 8) / 40]\n",
      "2024-12-23 18:33:09.162 Hail: INFO: merging 41 files totalling 6.7M...+ 7) / 40]\n",
      "2024-12-23 18:33:09.534 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Height_PRScs_hg38.bgz\n",
      "  merge time: 371.539ms\n",
      "2024-12-23 18:33:19.975 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:33:24.352 Hail: INFO: wrote table with 685740 rows in 1 partition to /tmp/__iruid_49786-iNs59KR50RfCKsGwtO6J6t\n",
      "2024-12-23 18:37:11.224 Hail: INFO: Coerced sorted dataset=>      (14 + 2) / 16]\n",
      "2024-12-23 18:37:12.524 Hail: INFO: merging 17 files totalling 6.7M...+ 7) / 16]\n",
      "2024-12-23 18:37:12.848 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/RBC_PRScs_hg38.bgz\n",
      "  merge time: 323.266ms\n",
      "2024-12-23 18:37:22.895 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:37:27.397 Hail: INFO: wrote table with 685739 rows in 1 partition to /tmp/__iruid_56675-RgeKThOIJkLS4QvLG0dl3F\n",
      "2024-12-23 18:41:16.488 Hail: INFO: Coerced sorted dataset        (32 + 8) / 40]\n",
      "2024-12-23 18:41:18.405 Hail: INFO: merging 41 files totalling 6.6M... 16) / 40]\n",
      "2024-12-23 18:41:18.711 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/TC_PRScs_hg38.bgz\n",
      "  merge time: 304.926ms\n",
      "2024-12-23 18:41:29.199 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:41:33.739 Hail: INFO: wrote table with 685739 rows in 1 partition to /tmp/__iruid_63564-1xcyRDtgpW6DlLX2KscZEJ\n",
      "2024-12-23 18:45:18.635 Hail: INFO: Coerced sorted dataset        (32 + 8) / 40]\n",
      "2024-12-23 18:45:19.696 Hail: INFO: merging 41 files totalling 6.7M...+ 8) / 40]\n",
      "2024-12-23 18:45:20.053 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/leukocyte_PRScs_hg38.bgz\n",
      "  merge time: 356.882ms\n",
      "2024-12-23 18:45:29.881 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:45:34.348 Hail: INFO: wrote table with 685753 rows in 1 partition to /tmp/__iruid_70335-dYJczZAmIMCY2zYUQXsR9l\n",
      "2024-12-23 18:49:06.116 Hail: INFO: Coerced sorted dataset        (40 + 8) / 48]\n",
      "2024-12-23 18:49:07.272 Hail: INFO: merging 49 files totalling 6.6M... 12) / 48]\n",
      "2024-12-23 18:49:07.702 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/T2D_PRScs_hg38.bgz\n",
      "  merge time: 430.291ms\n",
      "2024-12-23 18:49:17.405 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:49:21.639 Hail: INFO: wrote table with 685748 rows in 1 partition to /tmp/__iruid_77106-FBkjnjJkqcQl8tQtd5DYhE\n",
      "2024-12-23 18:53:01.537 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:53:02.172 Hail: INFO: merging 49 files totalling 6.6M...\n",
      "2024-12-23 18:53:02.477 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Asthma_PRScs_hg38.bgz\n",
      "  merge time: 305.424ms\n",
      "2024-12-23 18:53:11.889 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:53:16.219 Hail: INFO: wrote table with 685753 rows in 1 partition to /tmp/__iruid_83877-B7w8HGEYvGdtu2WDbpzlIT\n",
      "2024-12-23 18:56:57.704 Hail: INFO: Coerced sorted dataset        (40 + 8) / 48]\n",
      "2024-12-23 18:56:58.682 Hail: INFO: merging 49 files totalling 6.6M...+ 8) / 48]\n",
      "2024-12-23 18:56:59.025 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Breast_Cancer_PRScs_hg38.bgz\n",
      "  merge time: 343.709ms\n",
      "2024-12-23 18:57:08.762 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-23 18:57:13.406 Hail: INFO: wrote table with 685738 rows in 1 partition to /tmp/__iruid_90648-8EopDLHOlvk9OgaXbTEUR5\n",
      "2024-12-23 19:00:20.471 Hail: INFO: Coerced sorted dataset>       (48 + 8) / 56]\n",
      "2024-12-23 19:00:21.545 Hail: INFO: merging 57 files totalling 6.5M...+ 8) / 56]\n",
      "2024-12-23 19:00:21.891 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Colorectal_Cancer_PRScs_hg38.bgz\n",
      "  merge time: 345.172ms\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1894, in _run_once\n",
      "    handle = self._ready.popleft()\n",
      "IndexError: pop from an empty deque\n"
     ]
    }
   ],
   "source": [
    "export_Scores(mt_array_quant, \"DBP\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_quant, \"HDL\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_quant, \"Height\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_quant, \"RBC\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_quant, \"TC\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_quant, \"leukocyte\", \"Array\", \"PRScs_hg38\")\n",
    "\n",
    "export_Scores(mt_array_binary, \"T2D\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_binary, \"Asthma\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_binary, \"Breast_Cancer\", \"Array\", \"PRScs_hg38\")\n",
    "export_Scores(mt_array_binary, \"Colorectal_Cancer\", \"Array\", \"PRScs_hg38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/DBP_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/HDL_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Height_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/RBC_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/TC_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/leukocyte_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/T2D_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Asthma_PRScs_hg38.bgz\n",
      "CommandException: No URLs matched: gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Breast_Cancer_PRScs_hg38.bgz\n",
      "Copying gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Colorectal_Cancer_PRScs_hg38.bgz [Content-Type=application/octet-stream]...\n",
      "Removing gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Colorectal_Cancer_PRScs_hg38.bgz...\n",
      "\n",
      "Operation completed over 1 objects/6.5 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "base_path=\"gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array\"\n",
    "phenotypes=(\"DBP\" \"HDL\" \"Height\" \"RBC\" \"TC\" \"leukocyte\" \"T2D\" \"Asthma\" \"Breast_Cancer\" \"Colorectal_Cancer\")\n",
    "\n",
    "for phenotype in \"${phenotypes[@]}\"; do\n",
    "    gsutil mv \"${base_path}/${phenotype}_PRScs_hg38.bgz\" \"${base_path}/${phenotype}_PRScs_hg38.gz\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGS PGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-b.c.terra-vpc-sc-fd39b54c.internal:34577\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20250119-0156-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "# read matrix table \n",
    "mt_wgs = hl.read_matrix_table(f\"{bucket}/WGSData/WGS_GT_QCed.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 01:57:26.551 Hail: INFO: Reading table to impute column types 1) / 1]\n",
      "2025-01-19 01:57:37.712 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, BMI: float64, Height: float64, DBP: float64, SBP: float64, HbA1c: float64, leukocyte: float64, Lymphocyte: float64, RBC: float64, Neutrophil: float64, Hemoglobin_concentration: float64, hematocrit_percentage: float64, Eosinophil: float64, Platelet: float64, Monocyte: float64, MCV: float64, MCH: float64, Basophil: float64, MCHC: float64, HDL: float64, LDL: float64, TC: float64, TG: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  float64: 38\n",
      "  int32: 24\n",
      "  str: 1\n",
      "2025-01-19 01:57:39.310 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 01:57:42.675 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, Asthma: float64, Breast_Cancer: float64, CHD: float64, Colorectal_Cancer: int32, Prostate_Cancer: int32, T2D: float64, T1D: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  int32: 26\n",
      "  float64: 21\n",
      "  str: 1\n"
     ]
    }
   ],
   "source": [
    "## read Sample_quant \n",
    "Sample_quant = hl.import_table(f\"{bucket}/Pheno/quant_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_quant = Sample_quant.key_by(\"person_id\")\n",
    "mt_wgs_quant = mt_wgs.semi_join_cols(Sample_quant)\n",
    "mt_wgs_quant = mt_wgs_quant.annotate_cols(**Sample_quant[mt_wgs_quant.s])\n",
    "\n",
    "\n",
    "#### read Sample_binary\n",
    "Sample_binary = hl.import_table(f\"{bucket}/Pheno/binary_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_binary = Sample_binary.key_by(\"person_id\")\n",
    "mt_wgs_binary = mt_wgs.semi_join_cols(Sample_binary)\n",
    "mt_wgs_binary = mt_wgs_binary.annotate_cols(**Sample_binary[mt_wgs_binary.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Height\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:23:24.745 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 00:23:31.119 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: DBP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:23:32.281 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 00:23:38.474 Hail: INFO: Finished type imputation        (1 + 2) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: HDL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:23:39.605 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 00:23:45.856 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: TC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:23:46.977 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 00:23:53.036 Hail: INFO: Finished type imputation        (1 + 2) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: RBC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:23:53.990 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 00:23:59.918 Hail: INFO: Finished type imputation        (1 + 2) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: leukocyte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 00:24:00.820 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 00:24:06.930 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    }
   ],
   "source": [
    "# List of quant phenotypes\n",
    "phenotypes = [\"Height\", \"DBP\", \"HDL\", \"TC\", \"RBC\", \"leukocyte\"]\n",
    "\n",
    "# Loop through each phenotype and annotate it to the Matrix Table\n",
    "for phenotype in phenotypes:\n",
    "    print(f\"Processing phenotype: {phenotype}\")\n",
    "    \n",
    "    # Import the phenotype table\n",
    "    phenotype_table = hl.import_table(\n",
    "        f'{bucket}/PRScs/hg38/WGS_{phenotype}_pst_eff_a1_b0.5_phiauto.txt',\n",
    "        impute=True,\n",
    "        types = {\"locus\": hl.tlocus(\"GRCh38\"),\n",
    "                \"alleles\":hl.tarray(hl.tstr)}\n",
    "    )\n",
    "    \n",
    "    phenotype_table = phenotype_table.key_by('locus', 'alleles')\n",
    "    \n",
    "    # Annotate rows in the Matrix Table with the phenotype data\n",
    "    mt_wgs_quant = mt_wgs_quant.annotate_rows(\n",
    "        **{f\"{phenotype}_PRScs_result\": phenotype_table[mt_wgs_quant.locus, mt_wgs_quant.alleles]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: T2D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 01:57:43.865 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 01:57:50.178 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Asthma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 01:57:51.225 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 01:57:57.252 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Breast_Cancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 01:57:58.148 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 01:58:04.332 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing phenotype: Colorectal_Cancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 01:58:05.294 Hail: INFO: Reading table to impute column types\n",
      "2025-01-19 01:58:11.298 Hail: INFO: Finished type imputation        (2 + 1) / 3]\n",
      "  Loading field 'CHR' as type int32 (imputed)\n",
      "  Loading field 'ID' as type str (imputed)\n",
      "  Loading field 'BP' as type int32 (imputed)\n",
      "  Loading field 'A1_alt' as type str (imputed)\n",
      "  Loading field 'A2_ref' as type str (imputed)\n",
      "  Loading field 'BETA_posterior' as type float64 (imputed)\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied type)\n",
      "  Loading field 'alleles' as type array<str> (user-supplied type)\n"
     ]
    }
   ],
   "source": [
    "# List of binary phenotypes\n",
    "phenotypes = [\"T2D\", \"Asthma\", \"Breast_Cancer\", \"Colorectal_Cancer\"]\n",
    "\n",
    "# Loop through each phenotype and annotate it to the Matrix Table\n",
    "for phenotype in phenotypes:\n",
    "    print(f\"Processing phenotype: {phenotype}\")\n",
    "    \n",
    "    # Import the phenotype table\n",
    "    phenotype_table = hl.import_table(\n",
    "        f'{bucket}/PRScs/hg38/WGS_{phenotype}_pst_eff_a1_b0.5_phiauto.txt',\n",
    "        impute=True,\n",
    "        types = {\"locus\": hl.tlocus(\"GRCh38\"),\n",
    "                \"alleles\":hl.tarray(hl.tstr)}\n",
    "    )\n",
    "    \n",
    "    phenotype_table = phenotype_table.key_by('locus', 'alleles')\n",
    "    \n",
    "    # Annotate rows in the Matrix Table with the phenotype data\n",
    "    mt_wgs_binary = mt_wgs_binary.annotate_rows(\n",
    "        **{f\"{phenotype}_PRScs_result\": phenotype_table[mt_wgs_binary.locus, mt_wgs_binary.alleles]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PGS\n",
    "phenotypes = [\"Height\", \"DBP\", \"HDL\", \"TC\", \"RBC\", \"leukocyte\"]\n",
    "\n",
    "# Annotate PGS for all phenotypes\n",
    "mt_wgs_quant = mt_wgs_quant.annotate_cols(**{\n",
    "    f\"{phenotype}_pgs\": hl.struct(\n",
    "        pgs=hl.agg.sum(\n",
    "            mt_wgs_quant[f\"{phenotype}_PRScs_result\"][\"BETA_posterior\"] * mt_wgs_quant.GT\n",
    "        )\n",
    "    )\n",
    "    for phenotype in phenotypes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PGS\n",
    "phenotypes = [\"T2D\", \"Asthma\", \"Breast_Cancer\", \"Colorectal_Cancer\"]\n",
    "\n",
    "# Annotate PGS for all phenotypes\n",
    "mt_wgs_binary = mt_wgs_binary.annotate_cols(**{\n",
    "    f\"{phenotype}_pgs\": hl.struct(\n",
    "        pgs=hl.agg.sum(\n",
    "            mt_wgs_binary[f\"{phenotype}_PRScs_result\"][\"BETA_posterior\"] * mt_wgs_binary.GT\n",
    "        )\n",
    "    )\n",
    "    for phenotype in phenotypes\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Scores(mt, pheno, mt_type, method):\n",
    "    export_filename = f\"{bucket}/Scores/{mt_type}/{pheno}_{method}_checkpoint.ht\"  # Changed extension to .ht (Hail table)\n",
    "    sample_info = mt.cols().select(\n",
    "        \"Age\", 'is_sex_Male', 'is_sex_Female', \n",
    "        \"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \n",
    "        \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\", \"PC14\", \"PC15\", \"PC16\", \n",
    "        pheno + \"_pgs\", pheno)\n",
    "    sample_info = sample_info.annotate(**sample_info[pheno + \"_pgs\"])\n",
    "    sample_info = sample_info.drop(pheno + \"_pgs\")\n",
    "    \n",
    "    # Write the data as a Hail table (.ht)\n",
    "    sample_info = sample_info.checkpoint(export_filename, overwrite=True)  \n",
    "    print(f\"Wrote sample info to {export_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_Scores(mt_wgs_quant, \"DBP\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_quant, \"HDL\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_quant, \"Height\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_quant, \"RBC\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_quant, \"TC\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_quant, \"leukocyte\", \"WGS\", \"PRScs_hg38\")\n",
    "\n",
    "export_Scores(mt_wgs_binary, \"T2D\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_binary, \"Asthma\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_binary, \"Breast_Cancer\", \"WGS\", \"PRScs_hg38\")\n",
    "export_Scores(mt_wgs_binary, \"Colorectal_Cancer\", \"WGS\", \"PRScs_hg38\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-b.c.terra-vpc-sc-fd39b54c.internal:38651\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20250119-0235-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "Height_df = hl.read_table(f\"{bucket}/Scores/WGS/Height_PRScs_hg38_checkpoint.ht\")\n",
    "DBP_df = hl.read_table(f\"{bucket}/Scores/WGS/DBP_PRScs_hg38_checkpoint.ht\")\n",
    "HDL_df = hl.read_table(f\"{bucket}/Scores/WGS/HDL_PRScs_hg38_checkpoint.ht\")\n",
    "TC_df = hl.read_table(f\"{bucket}/Scores/WGS/TC_PRScs_hg38_checkpoint.ht\")\n",
    "RBC_df = hl.read_table(f\"{bucket}/Scores/WGS/RBC_PRScs_hg38_checkpoint.ht\")\n",
    "leukocyte_df = hl.read_table(f\"{bucket}/Scores/WGS/leukocyte_PRScs_hg38_checkpoint.ht\")\n",
    "T2D = hl.read_table(f\"{bucket}/Scores/WGS/T2D_PRScs_hg38_checkpoint.ht\")\n",
    "Asthma = hl.read_table(f\"{bucket}/Scores/WGS/Asthma_PRScs_hg38_checkpoint.ht\")\n",
    "Breast_Cancer = hl.read_table(f\"{bucket}/Scores/WGS/Breast_Cancer_PRScs_hg38_checkpoint.ht\")\n",
    "Colorectal_Cancer = hl.read_table(f\"{bucket}/Scores/WGS/Colorectal_Cancer_PRScs_hg38_checkpoint.ht\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 02:36:41.114 Hail: INFO: merging 161 files totalling 6.4M...6) / 160]\n",
      "2025-01-19 02:36:42.072 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/Height_PRScs_hg38.gz\n",
      "  merge time: 955.608ms\n",
      "2025-01-19 02:36:48.350 Hail: INFO: merging 161 files totalling 6.4M...8) / 160]\n",
      "2025-01-19 02:36:48.899 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/DBP_PRScs_hg38.gz\n",
      "  merge time: 548.911ms\n",
      "2025-01-19 02:36:54.137 Hail: INFO: merging 161 files totalling 6.3M...6) / 160]\n",
      "2025-01-19 02:36:54.641 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/HDL_PRScs_hg38.gz\n",
      "  merge time: 503.318ms\n",
      "2025-01-19 02:36:56.433 Hail: INFO: merging 161 files totalling 6.3M...1) / 160]\n",
      "2025-01-19 02:36:57.054 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/TC_PRScs_hg38.gz\n",
      "  merge time: 621.552ms\n",
      "2025-01-19 02:36:58.641 Hail: INFO: merging 161 files totalling 6.4M...\n",
      "2025-01-19 02:36:59.114 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/RBC_PRScs_hg38.gz\n",
      "  merge time: 471.980ms\n",
      "2025-01-19 02:37:00.630 Hail: INFO: merging 161 files totalling 6.4M...\n",
      "2025-01-19 02:37:01.227 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/leukocyte_PRScs_hg38.gz\n",
      "  merge time: 596.513ms\n",
      "2025-01-19 02:37:02.961 Hail: INFO: merging 161 files totalling 6.3M...2) / 160]\n",
      "2025-01-19 02:37:03.425 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/T2D_PRScs_hg38.gz\n",
      "  merge time: 463.609ms\n",
      "2025-01-19 02:37:04.912 Hail: INFO: merging 161 files totalling 6.3M...\n",
      "2025-01-19 02:37:05.415 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/Asthma_PRScs_hg38.gz\n",
      "  merge time: 502.711ms\n",
      "2025-01-19 02:37:06.837 Hail: INFO: merging 161 files totalling 6.3M...\n",
      "2025-01-19 02:37:07.292 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/Breast_Cancer_PRScs_hg38.gz\n",
      "  merge time: 454.473ms\n",
      "2025-01-19 02:37:08.768 Hail: INFO: merging 161 files totalling 6.2M...\n",
      "2025-01-19 02:37:09.174 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/Colorectal_Cancer_PRScs_hg38.gz\n",
      "  merge time: 405.467ms\n"
     ]
    }
   ],
   "source": [
    "Height_df.export(f\"{bucket}/Scores/WGS/Height_PRScs_hg38.gz\")\n",
    "DBP_df.export(f\"{bucket}/Scores/WGS/DBP_PRScs_hg38.gz\")\n",
    "HDL_df.export(f\"{bucket}/Scores/WGS/HDL_PRScs_hg38.gz\")\n",
    "TC_df.export(f\"{bucket}/Scores/WGS/TC_PRScs_hg38.gz\")\n",
    "RBC_df.export(f\"{bucket}/Scores/WGS/RBC_PRScs_hg38.gz\")\n",
    "leukocyte_df.export(f\"{bucket}/Scores/WGS/leukocyte_PRScs_hg38.gz\")\n",
    "T2D.export(f\"{bucket}/Scores/WGS/T2D_PRScs_hg38.gz\")\n",
    "Asthma.export(f\"{bucket}/Scores/WGS/Asthma_PRScs_hg38.gz\")\n",
    "Breast_Cancer.export(f\"{bucket}/Scores/WGS/Breast_Cancer_PRScs_hg38.gz\")\n",
    "Colorectal_Cancer.export(f\"{bucket}/Scores/WGS/Colorectal_Cancer_PRScs_hg38.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
