{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain\n",
    "import hail as hl\n",
    "from hail.linalg import BlockMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.getenv('WORKSPACE_CDR')\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QC\n",
    "\n",
    "1. Liftover, key the sumstats by locus  \n",
    "2. Restrict to biallelic variants  \n",
    "3. Negative strand problems  \n",
    "4. Annotate to matrix table, and swap ref:alt if necessary  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liftover, then key the sumstats by locus (done in Step2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rg37 = hl.get_reference('GRCh37')  \n",
    "rg38 = hl.get_reference('GRCh38')   \n",
    "rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-c.c.terra-vpc-sc-fd39b54c.internal:41389\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20241117-2009-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "# biallelic SNP after AFR-specific QC\n",
    "var_wgs_afr = hl.read_table(f'{bucket}/WGSData/WGS_Vars_Ancestry_AFR_QCed.ht')\n",
    "var_wgs_afr = var_wgs_afr.key_by(\"locus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantatative traits\n",
    "\n",
    "Use `beta_meta_hq`, `se_meta_hq`, `neglog10_pval_meta_hq`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add `var_wgs = var_wgs.repartition(2000)`, var_wgs = var_wgs.checkpoint(ht_filename_check`, overwrite=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2a_Array_SumStats_QC.ipynb performed some preliminary QC for sumstats\n",
    "# The intermediate file is saved as checkpoint\n",
    "# start from the checkpoint files\n",
    "\n",
    "def sumstats_QC_quant(ht_filename_in, ht_filename_check, filename_out, var_wgs):\n",
    "    sumstats = hl.read_table(ht_filename_in)\n",
    "    \n",
    "    ##############################################################################################################\n",
    "    ###################################### Analysis done in Step2a_Array_SumStats_QC.ipynb #######################\n",
    "    # most of the fields are irrelevant for our analysis, drop it first to save computation                      #\n",
    "    # biallelic SNPs                                                                                             #\n",
    "    # check if is_SNP                                                                                            #                                                                                   #\n",
    "    # flip negative strand                                                                                       #\n",
    "    ##############################################################################################################\n",
    "    ##############################################################################################################\n",
    "    \n",
    "    var_wgs = var_wgs.repartition(2000) ###***\n",
    "    \n",
    "    ############## check ref:alt, flip if necessary ############\n",
    "    var_wgs = var_wgs.annotate(**sumstats[var_wgs.locus])\n",
    "    var_wgs = var_wgs.filter((~hl.is_nan(var_wgs.beta_meta_hq)) &\n",
    "                                  (~hl.is_nan(var_wgs.se_meta_hq)) &\n",
    "                                  (~hl.is_nan(var_wgs.neglog10_pval_meta_hq)))\n",
    "    \n",
    "    var_wgs = var_wgs.annotate(beta_meta_fix_ref_alt = hl.case()\n",
    "        .when(((var_wgs.alleles[0] == var_wgs.alleles_fix_neg_strand[0]) & (var_wgs.alleles[1] == var_wgs.alleles_fix_neg_strand[1])), var_wgs.beta_meta_hq) \n",
    "        .when(((var_wgs.alleles[0] == var_wgs.alleles_fix_neg_strand[1]) & (var_wgs.alleles[1] == var_wgs.alleles_fix_neg_strand[0])), -var_wgs.beta_meta_hq)\n",
    "        .default(float('nan')))\n",
    "    \n",
    "    var_wgs = var_wgs.checkpoint(ht_filename_check, overwrite=True)\n",
    "    \n",
    "    sumstats_QCed = var_wgs.select(\n",
    "                        rsid = var_wgs.rsid,\n",
    "                        alleles1_wgs = var_wgs.alleles[0],\n",
    "                        alleles2_wgs = var_wgs.alleles[1],\n",
    "                        alleles1_sumstats_original = var_wgs.ref,\n",
    "                        alleles2_sumstats_original = var_wgs.alt,\n",
    "                        is_negative_strand = var_wgs.is_negative_strand,\n",
    "                        alleles1_sumstats_fixstrand = var_wgs.alleles_fix_neg_strand[0],\n",
    "                        alleles2_sumstats_fixstrand = var_wgs.alleles_fix_neg_strand[1],\n",
    "                        beta_meta = var_wgs.beta_meta_hq,\n",
    "                        beta_meta_fix_ref_alt = var_wgs.beta_meta_fix_ref_alt,\n",
    "                        se_meta = var_wgs.se_meta_hq,\n",
    "                        neglog10_pval_meta = var_wgs.neglog10_pval_meta_hq)\n",
    "    \n",
    "    sumstats_QCed.export(filename_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDL_AFR 2024-11-17 20:10:40.042188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred>(1990 + 10) / 2000]\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/usr/lib/spark/jars/spark-core_2.12-3.3.0.jar) to field java.util.concurrent.locks.ReentrantReadWriteLock.readerLock\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "2024-11-17 20:14:43.969 Hail: INFO: wrote table with 7621041 rows in 2000 partitions to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/biomarkers-30760-both_sexes-irnt_checkpoint3.ht\n",
      "2024-11-17 20:14:54.251 Hail: INFO: merging 2001 files totalling 127.0M... 2000]\n",
      "2024-11-17 20:14:59.672 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_HDL_afr_QCed.tsv.bgz\n",
      "  merge time: 5.420s\n"
     ]
    }
   ],
   "source": [
    "print(\"HDL_AFR \" + str(datetime.now()))\n",
    "sumstats_QC_quant(f\"{bucket}/Sumstats/biomarkers-30760-both_sexes-irnt_checkpoint2.ht\",\n",
    "                  f\"{bucket}/Sumstats/biomarkers-30760-both_sexes-irnt_checkpoint3.ht\",\n",
    "                  f\"{bucket}/Sumstats/WGS_HDL_afr_QCed.tsv.bgz\",\n",
    "                  var_wgs_afr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
