{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain\n",
    "import hail as hl\n",
    "from hail.linalg import BlockMatrix\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.getenv('WORKSPACE_CDR')\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read matrix table \n",
    "mt_array = hl.read_matrix_table(f\"{bucket}/ArrayData/Array_GT_QCed.mt\")\n",
    "mt_array = mt_array.key_rows_by(\"locus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 20:21:11.016 Hail: INFO: wrote table with 91490 rows in 1 partition to /tmp/persist_tablenjd78OgF2r\n",
      "2023-04-05 20:21:12.732 Hail: INFO: Reading table to impute column types\n",
      "2023-04-05 20:21:21.179 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, BMI: float64, Height: float64, DBP: float64, SBP: float64, HbA1c: float64, leukocyte: float64, Lymphocyte: float64, RBC: float64, Neutrophil: float64, Hemoglobin_concentration: float64, hematocrit_percentage: float64, Eosinophil: float64, Platelet: float64, Monocyte: float64, MCV: float64, MCH: float64, Basophil: float64, MCHC: float64, HDL: float64, LDL: float64, TC: float64, TG: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  float64: 38\n",
      "  int32: 24\n",
      "  str: 1\n",
      "2023-04-05 20:21:22.818 Hail: INFO: wrote table with 91631 rows in 1 partition to /tmp/persist_tableDWRbyK87mQ\n",
      "2023-04-05 20:21:24.539 Hail: INFO: Reading table to impute column types 1) / 1]\n",
      "2023-04-05 20:21:28.480 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, Asthma: float64, Breast_Cancer: float64, CHD: float64, Colorectal_Cancer: int32, Prostate_Cancer: int32, T2D: float64, T1D: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  int32: 26\n",
      "  float64: 21\n",
      "  str: 1\n"
     ]
    }
   ],
   "source": [
    "## read Sample_quant \n",
    "Sample_quant = hl.import_table(f\"{bucket}/Pheno/quant_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_quant = Sample_quant.key_by(\"person_id\")\n",
    "mt_array_quant = mt_array.semi_join_cols(Sample_quant)\n",
    "mt_array_quant = mt_array_quant.annotate_cols(**Sample_quant[mt_array_quant.s])\n",
    "\n",
    "\n",
    "#### read Sample_binary\n",
    "Sample_binary = hl.import_table(f\"{bucket}/Pheno/binary_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_binary = Sample_binary.key_by(\"person_id\")\n",
    "mt_array_binary = mt_array.semi_join_cols(Sample_binary)\n",
    "mt_array_binary = mt_array_binary.annotate_cols(**Sample_binary[mt_array_binary.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumStats_Wrangle(sumstats_file, phenotype):\n",
    "    \n",
    "    ## read hail table; change key\n",
    "    if phenotype == \"quant\":\n",
    "        sumstats = hl.import_table(sumstats_file,\n",
    "                         types={\"is_negative_strand\": \"bool\",\n",
    "                                \"af_meta_hq\": \"float\",\n",
    "                                \"beta_meta_hq\": \"float\",\n",
    "                                \"beta_meta_hq_fix_ref_alt\": \"float\",\n",
    "                                \"se_meta_hq\": \"float\",\n",
    "                                \"neglog10_pval_meta_hq\": \"float\",\n",
    "                                \"pval_meta_hq\": \"float\"})\n",
    "\n",
    "        sumstats = sumstats.rename({'af_meta_hq': 'af_meta', \n",
    "                                         'beta_meta_hq': 'beta_meta',\n",
    "                                         'beta_meta_hq_fix_ref_alt': 'beta_meta_fix_ref_alt',\n",
    "                                         'se_meta_hq': 'se_meta',\n",
    "                                         'neglog10_pval_meta_hq': 'neglog10_pval_meta',\n",
    "                                         'pval_meta_hq': 'pval_meta',})\n",
    "    elif phenotype == \"binary\":\n",
    "        sumstats = hl.import_table(sumstats_file,\n",
    "                     types={\"is_negative_strand\": \"bool\",\n",
    "                            \"beta_meta\": \"float\",\n",
    "                            \"beta_meta_fix_ref_alt\": \"float\",\n",
    "                            \"se_meta\": \"float\",\n",
    "                            \"neglog10_pval_meta\": \"float\",\n",
    "                            \"pval_meta\": \"float\"})\n",
    "        \n",
    "    sumstats = sumstats.annotate(locus = hl.parse_locus(sumstats.locus, reference_genome='GRCh38'))\n",
    "    sumstats = sumstats.key_by(\"locus\")\n",
    "    \n",
    "    ## add beta with different thresholds\n",
    "    sumstats = sumstats.annotate(beta_thresh1 =  sumstats.beta_meta_fix_ref_alt)\n",
    "    sumstats = sumstats.annotate(beta_thresh2 =  hl.if_else(sumstats.pval_meta < 0.5,  sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh3 =  hl.if_else(sumstats.pval_meta < 1e-1, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh4 =  hl.if_else(sumstats.pval_meta < 1e-2, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh5 =  hl.if_else(sumstats.pval_meta < 1e-3, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh6 =  hl.if_else(sumstats.pval_meta < 1e-4, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh7 =  hl.if_else(sumstats.pval_meta < 1e-5, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh8 =  hl.if_else(sumstats.pval_meta < 1e-6, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh9 =  hl.if_else(sumstats.pval_meta < 1e-7, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh10 = hl.if_else(sumstats.pval_meta < 5e-8, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    return(sumstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 20:21:30.053 Hail: INFO: wrote table with 261859 rows in 1 partition to /tmp/persist_tablenC0kn3wZm7\n",
      "2023-04-05 20:21:30.863 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta_hq' as type float64 (user-supplied)\n",
      "2023-04-05 20:21:32.392 Hail: INFO: wrote table with 261584 rows in 1 partition to /tmp/persist_table1AggeVS6vB\n",
      "2023-04-05 20:21:32.995 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta_hq' as type float64 (user-supplied)\n",
      "2023-04-05 20:21:34.960 Hail: INFO: wrote table with 262400 rows in 1 partition to /tmp/persist_tablesU2I6ASJjM\n",
      "2023-04-05 20:21:35.590 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta_hq' as type float64 (user-supplied)\n",
      "2023-04-05 20:21:37.278 Hail: INFO: wrote table with 262291 rows in 1 partition to /tmp/persist_tableAiU1ZC7y1S\n",
      "2023-04-05 20:21:37.792 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta_hq' as type float64 (user-supplied)\n",
      "2023-04-05 20:21:39.220 Hail: INFO: wrote table with 262107 rows in 1 partition to /tmp/persist_tableXAv2dx5qUN\n",
      "2023-04-05 20:21:39.834 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta_hq' as type float64 (user-supplied)\n",
      "2023-04-05 20:21:41.518 Hail: INFO: wrote table with 261712 rows in 1 partition to /tmp/persist_tablemzY53T2qS3\n",
      "2023-04-05 20:21:42.129 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta_hq' as type float64 (user-supplied)\n",
      "2023-04-05 20:21:46.275 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:21:52.194 Hail: INFO: wrote table with 261858 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/DBP_sumstats_checkpoint.ht\n",
      "2023-04-05 20:21:56.174 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:01.384 Hail: INFO: wrote table with 261583 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/HDL_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:04.589 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:10.125 Hail: INFO: wrote table with 262399 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/Height_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:13.078 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:18.372 Hail: INFO: wrote table with 262290 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/RBC_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:21.079 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:26.254 Hail: INFO: wrote table with 262106 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/TC_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:29.130 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:33.896 Hail: INFO: wrote table with 261711 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/leukocyte_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:35.494 Hail: INFO: wrote table with 262906 rows in 1 partition to /tmp/persist_tabletO4hd8g0vD\n",
      "2023-04-05 20:22:35.973 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta' as type float64 (user-supplied)\n",
      "2023-04-05 20:22:37.198 Hail: INFO: wrote table with 262805 rows in 1 partition to /tmp/persist_tableY1TNhBUKdv\n",
      "2023-04-05 20:22:37.676 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta' as type float64 (user-supplied)\n",
      "2023-04-05 20:22:39.242 Hail: INFO: wrote table with 262970 rows in 1 partition to /tmp/persist_tableTWeZlRFyi8\n",
      "2023-04-05 20:22:39.681 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta' as type float64 (user-supplied)\n",
      "2023-04-05 20:22:41.051 Hail: INFO: wrote table with 261536 rows in 1 partition to /tmp/persist_tableRQ9yjWbq3i\n",
      "2023-04-05 20:22:41.491 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta' as type float64 (user-supplied)\n",
      "2023-04-05 20:22:43.997 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:48.485 Hail: INFO: wrote table with 262905 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/T2D_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:51.183 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:22:56.020 Hail: INFO: wrote table with 262804 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/Asthma_sumstats_checkpoint.ht\n",
      "2023-04-05 20:22:58.528 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:23:03.868 Hail: INFO: wrote table with 262969 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/Breast_Cancer_sumstats_checkpoint.ht\n",
      "2023-04-05 20:23:06.674 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2023-04-05 20:23:11.555 Hail: INFO: wrote table with 261535 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/Colorectal_Cancer_sumstats_checkpoint.ht\n"
     ]
    }
   ],
   "source": [
    "######### quantatative sumstats #########\n",
    "DBP_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_DBP_QCed_clumped.tsv', \"quant\")\n",
    "HDL_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_HDL_QCed_clumped.tsv', \"quant\")\n",
    "Height_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_Height_QCed_clumped.tsv', \"quant\")\n",
    "RBC_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_RBC_QCed_clumped.tsv', \"quant\")\n",
    "TC_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_TC_QCed_clumped.tsv', \"quant\")\n",
    "leukocyte_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_leukocyte_QCed_clumped.tsv', \"quant\")\n",
    "# check point\n",
    "DBP_sumstats = DBP_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/DBP_sumstats_checkpoint.ht\")\n",
    "HDL_sumstats = HDL_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/HDL_sumstats_checkpoint.ht\")\n",
    "Height_sumstats = Height_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/Height_sumstats_checkpoint.ht\")\n",
    "RBC_sumstats = RBC_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/RBC_sumstats_checkpoint.ht\")\n",
    "TC_sumstats = TC_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/TC_sumstats_checkpoint.ht\")\n",
    "leukocyte_sumstats = leukocyte_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/leukocyte_sumstats_checkpoint.ht\")\n",
    "\n",
    "\n",
    "######### binary sumstats #########\n",
    "T2D_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_T2D_QCed_clumped.tsv', \"binary\")\n",
    "Asthma_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_Asthma_QCed_clumped.tsv', \"binary\")\n",
    "Breast_Cancer_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_Breast_Cancer_QCed_clumped.tsv', \"binary\")\n",
    "Colorectal_Cancer_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/Array_Colorectal_Cancer_QCed_clumped.tsv', \"binary\")\n",
    "# check point\n",
    "T2D_sumstats = T2D_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/T2D_sumstats_checkpoint.ht\")\n",
    "Asthma_sumstats = Asthma_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/Asthma_sumstats_checkpoint.ht\")\n",
    "Breast_Cancer_sumstats = Breast_Cancer_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/Breast_Cancer_sumstats_checkpoint.ht\")\n",
    "Colorectal_Cancer_sumstats = Colorectal_Cancer_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/Colorectal_Cancer_sumstats_checkpoint.ht\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 20:28:19.980 Hail: INFO: wrote matrix table with 975876 rows and 91489 columns in 74 partitions to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/mt_array_quant_checkpoint1.mt\n",
      "2023-04-05 20:32:02.525 Hail: INFO: wrote matrix table with 975876 rows and 91630 columns in 74 partitions to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/mt_array_binary_checkpoint1.mt\n"
     ]
    }
   ],
   "source": [
    "mt_array_quant = mt_array_quant.annotate_rows(\n",
    "                                  DBP_sumstats = DBP_sumstats[mt_array_quant.locus],\n",
    "                                  HDL_sumstats = HDL_sumstats[mt_array_quant.locus],\n",
    "                                  Height_sumstats = Height_sumstats[mt_array_quant.locus],\n",
    "                                  RBC_sumstats = RBC_sumstats[mt_array_quant.locus],\n",
    "                                  TC_sumstats = TC_sumstats[mt_array_quant.locus],\n",
    "                                  leukocyte_sumstats = leukocyte_sumstats[mt_array_quant.locus])\n",
    "\n",
    "mt_array_binary = mt_array_binary.annotate_rows(\n",
    "                                  T2D_sumstats = T2D_sumstats[mt_array_binary.locus],\n",
    "                                  Asthma_sumstats = Asthma_sumstats[mt_array_binary.locus],\n",
    "                                  Breast_Cancer_sumstats = Breast_Cancer_sumstats[mt_array_binary.locus],\n",
    "                                  Colorectal_Cancer_sumstats = Colorectal_Cancer_sumstats[mt_array_binary.locus])\n",
    "\n",
    "\n",
    "mt_array_quant = mt_array_quant.checkpoint(f\"{bucket}/hail_checkpoints/mt_array_quant_checkpoint1.mt\")\n",
    "mt_array_binary = mt_array_binary.checkpoint(f\"{bucket}/hail_checkpoints/mt_array_binary_checkpoint1.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_array_quant = mt_array_quant.annotate_cols(\n",
    "        # DBP\n",
    "        DBP_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh1 * mt_array_quant.GT),\n",
    "                  pgs2 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh2 * mt_array_quant.GT),\n",
    "                  pgs3 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh3 * mt_array_quant.GT),\n",
    "                  pgs4 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh4 * mt_array_quant.GT),\n",
    "                  pgs5 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh5 * mt_array_quant.GT),\n",
    "                  pgs6 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh6 * mt_array_quant.GT),\n",
    "                  pgs7 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh7 * mt_array_quant.GT),\n",
    "                  pgs8 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh8 * mt_array_quant.GT),\n",
    "                  pgs9 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh9 * mt_array_quant.GT),\n",
    "                  pgs10 = hl.agg.sum(mt_array_quant.DBP_sumstats.beta_thresh10 * mt_array_quant.GT)),\n",
    "        \n",
    "        # HDL\n",
    "        HDL_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh1 * mt_array_quant.GT),\n",
    "              pgs2 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh2 * mt_array_quant.GT),\n",
    "              pgs3 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh3 * mt_array_quant.GT),\n",
    "              pgs4 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh4 * mt_array_quant.GT),\n",
    "              pgs5 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh5 * mt_array_quant.GT),\n",
    "              pgs6 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh6 * mt_array_quant.GT),\n",
    "              pgs7 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh7 * mt_array_quant.GT),\n",
    "              pgs8 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh8 * mt_array_quant.GT),\n",
    "              pgs9 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh9 * mt_array_quant.GT),\n",
    "              pgs10 = hl.agg.sum(mt_array_quant.HDL_sumstats.beta_thresh10 * mt_array_quant.GT)),\n",
    "    \n",
    "        # Height\n",
    "        Height_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh1 * mt_array_quant.GT),\n",
    "              pgs2 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh2 * mt_array_quant.GT),\n",
    "              pgs3 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh3 * mt_array_quant.GT),\n",
    "              pgs4 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh4 * mt_array_quant.GT),\n",
    "              pgs5 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh5 * mt_array_quant.GT),\n",
    "              pgs6 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh6 * mt_array_quant.GT),\n",
    "              pgs7 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh7 * mt_array_quant.GT),\n",
    "              pgs8 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh8 * mt_array_quant.GT),\n",
    "              pgs9 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh9 * mt_array_quant.GT),\n",
    "              pgs10 = hl.agg.sum(mt_array_quant.Height_sumstats.beta_thresh10 * mt_array_quant.GT)),\n",
    "        \n",
    "        # RBC\n",
    "        RBC_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh1 * mt_array_quant.GT),\n",
    "              pgs2 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh2 * mt_array_quant.GT),\n",
    "              pgs3 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh3 * mt_array_quant.GT),\n",
    "              pgs4 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh4 * mt_array_quant.GT),\n",
    "              pgs5 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh5 * mt_array_quant.GT),\n",
    "              pgs6 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh6 * mt_array_quant.GT),\n",
    "              pgs7 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh7 * mt_array_quant.GT),\n",
    "              pgs8 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh8 * mt_array_quant.GT),\n",
    "              pgs9 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh9 * mt_array_quant.GT),\n",
    "              pgs10 = hl.agg.sum(mt_array_quant.RBC_sumstats.beta_thresh10 * mt_array_quant.GT)),\n",
    "        \n",
    "        # TC\n",
    "        TC_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh1 * mt_array_quant.GT),\n",
    "              pgs2 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh2 * mt_array_quant.GT),\n",
    "              pgs3 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh3 * mt_array_quant.GT),\n",
    "              pgs4 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh4 * mt_array_quant.GT),\n",
    "              pgs5 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh5 * mt_array_quant.GT),\n",
    "              pgs6 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh6 * mt_array_quant.GT),\n",
    "              pgs7 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh7 * mt_array_quant.GT),\n",
    "              pgs8 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh8 * mt_array_quant.GT),\n",
    "              pgs9 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh9 * mt_array_quant.GT),\n",
    "              pgs10 = hl.agg.sum(mt_array_quant.TC_sumstats.beta_thresh10 * mt_array_quant.GT)),\n",
    "        \n",
    "        # leukocyte\n",
    "        leukocyte_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh1 * mt_array_quant.GT),\n",
    "              pgs2 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh2 * mt_array_quant.GT),\n",
    "              pgs3 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh3 * mt_array_quant.GT),\n",
    "              pgs4 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh4 * mt_array_quant.GT),\n",
    "              pgs5 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh5 * mt_array_quant.GT),\n",
    "              pgs6 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh6 * mt_array_quant.GT),\n",
    "              pgs7 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh7 * mt_array_quant.GT),\n",
    "              pgs8 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh8 * mt_array_quant.GT),\n",
    "              pgs9 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh9 * mt_array_quant.GT),\n",
    "              pgs10 = hl.agg.sum(mt_array_quant.leukocyte_sumstats.beta_thresh10 * mt_array_quant.GT))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_array_binary = mt_array_binary.annotate_cols(\n",
    "        # T2D\n",
    "        T2D_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh1 * mt_array_binary.GT),\n",
    "                  pgs2 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh2 * mt_array_binary.GT),\n",
    "                  pgs3 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh3 * mt_array_binary.GT),\n",
    "                  pgs4 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh4 * mt_array_binary.GT),\n",
    "                  pgs5 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh5 * mt_array_binary.GT),\n",
    "                  pgs6 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh6 * mt_array_binary.GT),\n",
    "                  pgs7 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh7 * mt_array_binary.GT),\n",
    "                  pgs8 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh8 * mt_array_binary.GT),\n",
    "                  pgs9 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh9 * mt_array_binary.GT),\n",
    "                  pgs10 = hl.agg.sum(mt_array_binary.T2D_sumstats.beta_thresh10 * mt_array_binary.GT)),\n",
    "    \n",
    "        # Asthma\n",
    "        Asthma_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh1 * mt_array_binary.GT),\n",
    "                  pgs2 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh2 * mt_array_binary.GT),\n",
    "                  pgs3 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh3 * mt_array_binary.GT),\n",
    "                  pgs4 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh4 * mt_array_binary.GT),\n",
    "                  pgs5 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh5 * mt_array_binary.GT),\n",
    "                  pgs6 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh6 * mt_array_binary.GT),\n",
    "                  pgs7 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh7 * mt_array_binary.GT),\n",
    "                  pgs8 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh8 * mt_array_binary.GT),\n",
    "                  pgs9 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh9 * mt_array_binary.GT),\n",
    "                  pgs10 = hl.agg.sum(mt_array_binary.Asthma_sumstats.beta_thresh10 * mt_array_binary.GT)),\n",
    "    \n",
    "        # Breast_Cancer\n",
    "        Breast_Cancer_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh1 * mt_array_binary.GT),\n",
    "                  pgs2 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh2 * mt_array_binary.GT),\n",
    "                  pgs3 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh3 * mt_array_binary.GT),\n",
    "                  pgs4 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh4 * mt_array_binary.GT),\n",
    "                  pgs5 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh5 * mt_array_binary.GT),\n",
    "                  pgs6 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh6 * mt_array_binary.GT),\n",
    "                  pgs7 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh7 * mt_array_binary.GT),\n",
    "                  pgs8 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh8 * mt_array_binary.GT),\n",
    "                  pgs9 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh9 * mt_array_binary.GT),\n",
    "                  pgs10 = hl.agg.sum(mt_array_binary.Breast_Cancer_sumstats.beta_thresh10 * mt_array_binary.GT)),\n",
    "    \n",
    "    \n",
    "        # Colorectal_Cancer\n",
    "        Colorectal_Cancer_pgs = hl.struct(pgs1 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh1 * mt_array_binary.GT),\n",
    "                  pgs2 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh2 * mt_array_binary.GT),\n",
    "                  pgs3 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh3 * mt_array_binary.GT),\n",
    "                  pgs4 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh4 * mt_array_binary.GT),\n",
    "                  pgs5 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh5 * mt_array_binary.GT),\n",
    "                  pgs6 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh6 * mt_array_binary.GT),\n",
    "                  pgs7 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh7 * mt_array_binary.GT),\n",
    "                  pgs8 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh8 * mt_array_binary.GT),\n",
    "                  pgs9 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh9 * mt_array_binary.GT),\n",
    "                  pgs10 = hl.agg.sum(mt_array_binary.Colorectal_Cancer_sumstats.beta_thresh10 * mt_array_binary.GT))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Scores(mt, pheno, mt_type, method):\n",
    "    export_filename = f\"{bucket}/Scores/\" + mt_type + \"/\" + pheno + \"_\" + method + \".bgz\"\n",
    "    sample_info = mt.cols().select(\n",
    "        \"Age\", 'is_sex_Male', 'is_sex_Female', \n",
    "        \"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \n",
    "        \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\", \"PC14\", \"PC15\", \"PC16\", \n",
    "        pheno + \"_pgs\", pheno)\n",
    "    sample_info = sample_info.annotate(**sample_info[pheno + \"_pgs\"])\n",
    "    sample_info = sample_info.drop(pheno + \"_pgs\")\n",
    "    sample_info.export(export_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 20:32:33.481 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n",
      "2023-04-05 20:38:27.388 Hail: INFO: Coerced sorted dataset        (10 + 6) / 16]\n",
      "2023-04-05 20:38:31.080 Hail: INFO: merging 17 files totalling 10.0M... 1) / 16]\n",
      "2023-04-05 20:38:31.455 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/DBP_clump.bgz\n",
      "  merge time: 373.620ms\n",
      "2023-04-05 20:44:06.327 Hail: INFO: Coerced sorted dataset=>      (14 + 2) / 16]\n",
      "2023-04-05 20:44:09.544 Hail: INFO: merging 17 files totalling 9.8M...+ 1) / 16]\n",
      "2023-04-05 20:44:09.839 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/HDL_clump.bgz\n",
      "  merge time: 294.928ms\n",
      "2023-04-05 20:49:36.078 Hail: INFO: Coerced sorted dataset====>   (15 + 1) / 16]\n",
      "2023-04-05 20:49:38.262 Hail: INFO: merging 17 files totalling 10.0M... 1) / 16]\n",
      "2023-04-05 20:49:38.612 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/Height_clump.bgz\n",
      "  merge time: 350.258ms\n",
      "2023-04-05 20:54:58.664 Hail: INFO: Coerced sorted dataset          (1 + 1) / 2]\n",
      "2023-04-05 20:55:00.131 Hail: INFO: merging 17 files totalling 10.0M... 1) / 16]\n",
      "2023-04-05 20:55:00.488 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/RBC_clump.bgz\n",
      "  merge time: 356.905ms\n",
      "2023-04-05 21:00:36.134 Hail: INFO: Coerced sorted dataset=>      (14 + 2) / 16]\n",
      "2023-04-05 21:00:38.408 Hail: INFO: merging 17 files totalling 9.7M...+ 1) / 16]\n",
      "2023-04-05 21:00:38.730 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/Array/TC_clump.bgz\n",
      "  merge time: 321.328ms\n",
      "[Stage 72:==============================================>        (63 + 11) / 74]\r"
     ]
    }
   ],
   "source": [
    "export_Scores(mt_array_quant, \"DBP\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_quant, \"HDL\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_quant, \"Height\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_quant, \"RBC\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_quant, \"TC\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_quant, \"leukocyte\", \"Array\", \"clump\")\n",
    "\n",
    "export_Scores(mt_array_binary, \"T2D\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_binary, \"Asthma\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_binary, \"Breast_Cancer\", \"Array\", \"clump\")\n",
    "export_Scores(mt_array_binary, \"Colorectal_Cancer\", \"Array\", \"clump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mv f'{bucket}/Scores/Array/DBP_clump.bgz' f'{bucket}/Scores/Array/DBP_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/HDL_clump.bgz' f'{bucket}/Scores/Array/HDL_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/Height_clump.bgz' f'{bucket}/Scores/Array/Height_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/RBC_clump.bgz' f'{bucket}/Scores/Array/RBC_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/TC_clump.bgz' f'{bucket}/Scores/Array/TC_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/leukocyte_clump.bgz' f'{bucket}/Scores/Array/leukocyte_clump.gz'\n",
    "\n",
    "!gsutil mv f'{bucket}/Scores/Array/T2D_clump.bgz' f'{bucket}/Scores/Array/T2D_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/Asthma_clump.bgz' f'{bucket}/Scores/Array/Asthma_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/Breast_Cancer_clump.bgz' f'{bucket}/Scores/Array/Breast_Cancer_clump.gz'\n",
    "!gsutil mv f'{bucket}/Scores/Array/Colorectal_Cancer_clump.bgz' f'{bucket}/Scores/Array/Colorectal_Cancer_clump.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
