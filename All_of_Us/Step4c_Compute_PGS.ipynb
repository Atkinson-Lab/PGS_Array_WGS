{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import chain\n",
    "import hail as hl\n",
    "from hail.linalg import BlockMatrix\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/hail/context.py:352: UserWarning:\n",
      "\n",
      "Using hl.init with a default_reference argument is deprecated. To set a default reference genome after initializing hail, call `hl.default_reference` with an argument to set the default reference genome.\n",
      "\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-b.c.terra-vpc-sc-fd39b54c.internal:43339\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20241119-1146-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "# set up hail\n",
    "hl.init(default_reference = \"GRCh38\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read matrix table (from Supp_WGS_Ancestry_QC)\n",
    "mt_wgs_afr = hl.read_matrix_table(f\"{bucket}/WGSData/WGS_GT_HDL_Ancestry_AFR_QCed.mt\")\n",
    "mt_wgs_afr = mt_wgs_afr.key_rows_by(\"locus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 11:18:05.176 Hail: INFO: Reading table to impute column types\n",
      "2024-11-19 11:18:09.895 Hail: INFO: Loading <StructExpression of type struct{person_id: str, Age: int32, is_sex_Male: int32, is_sex_Female: int32, is_White: int32, is_Black_or_African_American: int32, is_Native_Hawaiian_or_Other_Pacific_Islander: int32, is_Asian: int32, is_Middle_Eastern_or_North_African: int32, is_gender_Male: int32, is_gender_Female: int32, is_Hispanic: int32, is_anc_pred_eur: int32, is_anc_pred_amr: int32, is_anc_pred_afr: int32, is_anc_pred_sas: int32, is_anc_pred_eas: int32, is_anc_pred_mid: int32, PC1: float64, PC2: float64, PC3: float64, PC4: float64, PC5: float64, PC6: float64, PC7: float64, PC8: float64, PC9: float64, PC10: float64, PC11: float64, PC12: float64, PC13: float64, PC14: float64, PC15: float64, PC16: float64, BMI: float64, Height: float64, DBP: float64, SBP: float64, HbA1c: float64, leukocyte: float64, Lymphocyte: float64, RBC: float64, Neutrophil: float64, Hemoglobin_concentration: float64, hematocrit_percentage: float64, Eosinophil: float64, Platelet: float64, Monocyte: float64, MCV: float64, MCH: float64, Basophil: float64, MCHC: float64, HDL: float64, LDL: float64, TC: float64, TG: float64, eur_bin: int32, amr_bin: int32, afr_bin: int32, oth_bin: int32, sas_bin: int32, eas_bin: int32, mid_bin: int32}> fields. Counts by type:\n",
      "  float64: 38\n",
      "  int32: 24\n",
      "  str: 1\n"
     ]
    }
   ],
   "source": [
    "## read Sample_quant \n",
    "Sample_quant = hl.import_table(f\"{bucket}/Pheno/quant_all.tsv\", \n",
    "                         missing='',\n",
    "                        impute=True,\n",
    "                        types = {\"person_id\": \"str\"})\n",
    "Sample_quant = Sample_quant.key_by(\"person_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_wgs_quant = mt_wgs_afr.semi_join_cols(Sample_quant)\n",
    "mt_wgs_quant = mt_wgs_quant.annotate_cols(**Sample_quant[mt_wgs_quant.s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumStats_Wrangle(sumstats_file, phenotype):\n",
    "    \n",
    "    ## read hail table; change key\n",
    "    sumstats = hl.import_table(sumstats_file,\n",
    "                 types={\"is_negative_strand\": \"bool\",\n",
    "                        \"beta_meta\": \"float\",\n",
    "                        \"beta_meta_fix_ref_alt\": \"float\",\n",
    "                        \"se_meta\": \"float\",\n",
    "                        \"neglog10_pval_meta\": \"float\",\n",
    "                        \"pval_meta\": \"float\"},\n",
    "                        missing=\"\") # Treat empty strings as missing\n",
    "    \n",
    "    sumstats = sumstats.annotate(locus = hl.parse_locus(sumstats.locus, reference_genome='GRCh38'))\n",
    "    sumstats = sumstats.key_by(\"locus\")\n",
    "    \n",
    "    ## add beta with different thresholds\n",
    "    sumstats = sumstats.annotate(beta_thresh1 =  sumstats.beta_meta_fix_ref_alt)\n",
    "    sumstats = sumstats.annotate(beta_thresh2 =  hl.if_else(sumstats.pval_meta < 0.5,  sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh3 =  hl.if_else(sumstats.pval_meta < 1e-1, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh4 =  hl.if_else(sumstats.pval_meta < 1e-2, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh5 =  hl.if_else(sumstats.pval_meta < 1e-3, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh6 =  hl.if_else(sumstats.pval_meta < 1e-4, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh7 =  hl.if_else(sumstats.pval_meta < 1e-5, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh8 =  hl.if_else(sumstats.pval_meta < 1e-6, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh9 =  hl.if_else(sumstats.pval_meta < 1e-7, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    sumstats = sumstats.annotate(beta_thresh10 = hl.if_else(sumstats.pval_meta < 5e-8, sumstats.beta_meta_fix_ref_alt, 0))\n",
    "    return(sumstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 21:42:24.123 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type str (not specified)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type bool (user-supplied)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type float64 (user-supplied)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type float64 (user-supplied)\n",
      "  Loading field 'se_meta' as type float64 (user-supplied)\n",
      "  Loading field 'neglog10_pval_meta' as type float64 (user-supplied)\n",
      "  Loading field 'varid' as type str (not specified)\n",
      "  Loading field 'pval_meta' as type float64 (user-supplied)\n",
      "2024-11-18 21:42:27.803 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-18 21:42:33.944 Hail: INFO: wrote table with 382342 rows in 1 partition to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/HDL_clumps_sumstats_afr_checkpoint.ht\n"
     ]
    }
   ],
   "source": [
    "HDL_sumstats = SumStats_Wrangle(f'{bucket}/Sumstats_clumped/WGS_HDL_afr_QCed_clumps.tsv', \"quant\") \n",
    "# check point\n",
    "HDL_sumstats = HDL_sumstats.checkpoint(f\"{bucket}/hail_checkpoints/HDL_clumps_sumstats_afr_checkpoint.ht\", overwrite=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read checkpoint\n",
    "HDL_sumstats = hl.read_table(f\"{bucket}/hail_checkpoints/HDL_clumps_sumstats_afr_checkpoint.ht\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 11:22:32.549 Hail: INFO: wrote table with 382342 rows in 1 partition to /tmp/__iruid_6005-AuIbzN5AVxKhokyrc9GJNJ\n",
      "2024-11-19 11:43:37.188 Hail: INFO: wrote matrix table with 382342 rows and 20652 columns in 140126 partitions to gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/hail_checkpoints/mt_wgs_quant_checkpoint2.mt\n"
     ]
    }
   ],
   "source": [
    "mt_wgs_quant = mt_wgs_quant.annotate_rows(HDL_sumstats = HDL_sumstats[mt_wgs_quant.locus])\n",
    "mt_wgs_quant = mt_wgs_quant.checkpoint(f\"{bucket}/hail_checkpoints/mt_wgs_quant_checkpoint2.mt\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start from here after writing the checkpoint MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_wgs_quant = hl.read_matrix_table(f\"{bucket}/hail_checkpoints/mt_wgs_quant_checkpoint2.mt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_wgs_quant = mt_wgs_quant.repartition(1000) ### this seems to dramatically speed up the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgs_fields = [f\"beta_thresh{i}\" for i in range(1, 11)]\n",
    "\n",
    "mt_wgs_quant = mt_wgs_quant.annotate_cols(\n",
    "    # HDL\n",
    "    HDL_pgs = hl.struct(**{\n",
    "        f\"pgs{i}\": hl.agg.sum(mt_wgs_quant.HDL_sumstats[pgs_fields[i-1]] * mt_wgs_quant.GT)\n",
    "        for i in range(1, 11)\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Scores(mt, pheno, mt_type, method, ancestry):\n",
    "    export_filename = f\"{bucket}/Scores/\" + mt_type + \"/\" + pheno + \"_\" + method + \"_\" + ancestry + \".bgz\"\n",
    "    sample_info = mt.cols().select(\n",
    "        \"Age\", 'is_sex_Male', 'is_sex_Female', \n",
    "        \"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \n",
    "        \"PC9\", \"PC10\", \"PC11\", \"PC12\", \"PC13\", \"PC14\", \"PC15\", \"PC16\", \n",
    "        pheno + \"_pgs\", pheno)\n",
    "    sample_info = sample_info.annotate(**sample_info[pheno + \"_pgs\"])\n",
    "    sample_info = sample_info.drop(pheno + \"_pgs\")\n",
    "    sample_info.export(export_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDL 2024-11-19 11:46:33.454331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 11:46:33.455 Hail: WARN: cols(): Resulting column table is sorted by 'col_key'.\n",
      "    To preserve matrix table column order, first unkey columns with 'key_cols_by()'\n",
      "2024-11-19 11:51:51.626 Hail: INFO: Coerced sorted dataset       (19 + 12) / 31]\n",
      "2024-11-19 11:51:53.662 Hail: INFO: merging 137 files totalling 2.3M...3) / 136]\n",
      "2024-11-19 11:51:54.268 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Scores/WGS/HDL_clump_AFR.bgz\n",
      "  merge time: 604.346ms\n"
     ]
    }
   ],
   "source": [
    "###### \n",
    "print(\"HDL \" + str(datetime.now()))\n",
    "export_Scores(mt_wgs_quant, \"HDL\", \"WGS\", \"clump\", \"AFR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mv f'{bucket}/Scores/WGS/HDL_clump_AFR.bgz' f'{bucket}/Scores/WGS/HDL_clump_AFR.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
