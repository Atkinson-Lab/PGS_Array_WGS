{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = os.getenv('WORKSPACE_CDR')\n",
    "bucket = os.getenv('WORKSPACE_BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Compute overlap with hm3 variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# downloaded from https://zenodo.org/records/7773502\n",
    "!gunzip w_hm3.snplist.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hm3 = pd.read_csv(\"w_hm3.snplist\", sep='\\t') #1217311  vairants\n",
    "qc_aou_array = pd.read_csv(\"Array_Vars_QCed_dbsnp.tsv\", sep='\\t') #975876 variants\n",
    "qc_aou_wgs = pd.read_csv(\"WGS_Vars_QCed_dbsnp.tsv\", sep='\\t') #8996707 variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "overlap = hm3[hm3['SNP'].isin(qc_aou_array['rsid'])] #297119 overalp variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30446388680529085"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "297119/975876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2440781361541956"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "297119/1217311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "overlap2 = qc_aou_array[qc_aou_array['rsid'].isin(qc_aou_array['rsid'])] #975876 overalp variants (100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Array QC'd locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-b.c.terra-vpc-sc-fd39b54c.internal:40693\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20241128-0048-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "var_array = hl.read_table(f\"{bucket}/ArrayData/Array_Vars_QCed.ht\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_array = var_array.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "db = hl.experimental.DB(region='us-central1', cloud='gcp')\n",
    "var_array = db.annotate_rows_db(var_array, 'dbSNP_rsid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the format of array<struct{rsid: str}> to a delimited string\n",
    "var_array = var_array.annotate(\n",
    "    dbSNP_rsid=hl.delimit(var_array.dbSNP_rsid.map(lambda x: x.rsid), delimiter=\",\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add ref and alt as seperate columns \n",
    "var_array = var_array.annotate(ref = var_array.alleles[0])\n",
    "var_array = var_array.annotate(alt = var_array.alleles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#var_array.count() #975876"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_array = var_array.key_by('locus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select desired cols for output \n",
    "varinfo_out = var_array.select(rsid = var_array.dbSNP_rsid,\n",
    "                               ref = var_array.ref,\n",
    "                               alt = var_array.alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 00:50:50.355 Hail: INFO: merging 100 files totalling 8.8M...1) / 100]\n",
      "2024-11-28 00:50:51.109 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/ArrayData/Array_Vars_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 752.272ms\n",
      "Traceback (most recent call last):\n"
     ]
    }
   ],
   "source": [
    "varinfo_out.export(f\"{bucket}/ArrayData/Array_Vars_QCed_dbsnp.tsv.bgz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## WGS QC'd locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n",
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-b.c.terra-vpc-sc-fd39b54c.internal:37287\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20241128-0055-0.2.130.post1-c69cd67afb8b.log\n"
     ]
    }
   ],
   "source": [
    "var_wgs = hl.read_table(f\"{bucket}/WGSData/WGS_Vars_QCed.ht\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_wgs = var_wgs.repartition(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "db = hl.experimental.DB(region='us-central1', cloud='gcp')\n",
    "var_wgs = db.annotate_rows_db(var_wgs, 'dbSNP_rsid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8996707"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#var_wgs.count() #8996707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify the format of wgs<struct{rsid: str}> to a delimited string\n",
    "var_wgs = var_wgs.annotate(\n",
    "    dbSNP_rsid=hl.delimit(var_wgs.dbSNP_rsid.map(lambda x: x.rsid), delimiter=\",\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add ref and alt as seperate columns \n",
    "var_wgs = var_wgs.annotate(ref = var_wgs.alleles[0])\n",
    "var_wgs = var_wgs.annotate(alt = var_wgs.alleles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "var_wgs = var_wgs.key_by('locus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select desired cols for output \n",
    "varinfo_out = var_wgs.select(rsid = var_wgs.dbSNP_rsid,\n",
    "                             ref = var_wgs.ref,\n",
    "                             alt = var_wgs.alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:14:10.374 Hail: INFO: merging 1000 files totalling 76.4M.../ 1000]\n",
      "2024-11-28 01:15:14.056 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/WGSData/WGS_Vars_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 1m3.7s\n"
     ]
    }
   ],
   "source": [
    "varinfo_out.export(f\"{bucket}/WGSData/WGS_Vars_QCed_dbsnp.tsv.bgz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Array QC'd sumstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sumstats_QC(filename_in, filename_out):\n",
    "    sumstats = hl.import_table(filename_in, \n",
    "                               #impute=True, \n",
    "                               types = {\"locus\": hl.tlocus(\"GRCh38\")})\n",
    "    # Add a new column \"alleles\" by combining \"alleles1_sumstats_fixstrand\" and \"alleles2_sumstats_fixstrand\"\n",
    "    sumstats = sumstats.annotate(\n",
    "    alleles=[sumstats.alleles1_sumstats_fixstrand, sumstats.alleles2_sumstats_fixstrand]\n",
    "    )\n",
    "    # Set the type of \"alleles\" explicitly to array<str> (Hail infers it automatically in this case)\n",
    "    sumstats = sumstats.annotate(\n",
    "    alleles=hl.array(sumstats.alleles)\n",
    "    )\n",
    "\n",
    "    sumstats = sumstats.key_by('locus', 'alleles') #for annotate dbSNP\n",
    "    sumstats = sumstats.repartition(100)\n",
    "    \n",
    "    # annotate dbSNP rsID \n",
    "    db = hl.experimental.DB(region='us-central1', cloud='gcp')\n",
    "    sumstats = db.annotate_rows_db(sumstats, 'dbSNP_rsid')\n",
    "    \n",
    "    # Simplify the format of wgs<struct{rsid: str}> to a delimited string\n",
    "    sumstats = sumstats.annotate(\n",
    "    dbSNP_rsid=hl.delimit(sumstats.dbSNP_rsid.map(lambda x: x.rsid), delimiter=\",\")\n",
    "    )\n",
    "    \n",
    "    # only keep required columns\n",
    "    sumstats = sumstats.key_by('locus') \n",
    "    sumstats = sumstats.select(\n",
    "    'dbSNP_rsid',   \n",
    "    'alleles2_sumstats_fixstrand',\n",
    "    'alleles1_sumstats_fixstrand',\n",
    "    'beta_meta_hq_fix_ref_alt',\n",
    "    'neglog10_pval_meta_hq'\n",
    "    )\n",
    "    \n",
    "    sumstats.export(filename_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height 2024-11-28 01:45:51.740745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:45:52.782 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta_hq' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type str (not specified)\n",
      "2024-11-28 01:45:59.897 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 01:47:20.010 Hail: INFO: merging 100 files totalling 15.2M...) / 100]\n",
      "2024-11-28 01:47:20.684 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_Height_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 674.365ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBP 2024-11-28 01:47:20.689098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:47:22.093 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta_hq' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type str (not specified)\n",
      "2024-11-28 01:47:33.585 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 01:48:45.489 Hail: INFO: merging 100 files totalling 15.2M...) / 100]\n",
      "2024-11-28 01:48:46.030 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_DBP_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 540.918ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDL 2024-11-28 01:48:46.034473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:48:46.993 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta_hq' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type str (not specified)\n",
      "2024-11-28 01:48:53.631 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 01:50:04.533 Hail: INFO: merging 100 files totalling 15.1M...) / 100]\n",
      "2024-11-28 01:50:05.093 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_HDL_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 559.738ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TC 2024-11-28 01:50:05.097208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:50:06.058 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta_hq' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type str (not specified)\n",
      "2024-11-28 01:50:12.866 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 01:51:24.203 Hail: INFO: merging 100 files totalling 15.1M...) / 100]\n",
      "2024-11-28 01:51:24.842 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_TC_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 639.281ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBC 2024-11-28 01:51:24.847224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:51:25.698 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta_hq' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type str (not specified)\n",
      "2024-11-28 01:51:32.473 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 01:52:43.748 Hail: INFO: merging 100 files totalling 15.2M...) / 100]\n",
      "2024-11-28 01:52:44.312 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_RBC_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 563.185ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leukocyte 2024-11-28 01:52:44.316448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:52:45.165 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'af_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq' as type str (not specified)\n",
      "  Loading field 'beta_meta_hq_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta_hq' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta_hq' as type str (not specified)\n",
      "2024-11-28 01:52:51.844 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 01:54:02.525 Hail: INFO: merging 100 files totalling 15.1M...) / 100]\n",
      "2024-11-28 01:54:03.073 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_leukocyte_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 547.893ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2D 2024-11-28 01:54:03.077284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 01:54:03.961 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Table instance has no field 'beta_meta_hq_fix_ref_alt'\n    Did you mean:\n        'beta_meta_fix_ref_alt' [row]\n    Hint: use 'describe()' to show the names of all data fields.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m sumstats_QC(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Sumstats/Array_leukocyte_QCed.tsv.bgz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Sumstats/Array_leukocyte_QCed_dbsnp.tsv.bgz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT2D \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[0;32m---> 26\u001b[0m \u001b[43msumstats_QC\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/Sumstats/Array_T2D_QCed.tsv.bgz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/Sumstats/Array_T2D_QCed_dbsnp.tsv.bgz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsthma \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mnow()))\n\u001b[1;32m     30\u001b[0m sumstats_QC(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Sumstats/Array_Asthma_QCed.tsv.bgz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/Sumstats/Array_Asthma_QCed_dbsnp.tsv.bgz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m, in \u001b[0;36msumstats_QC\u001b[0;34m(filename_in, filename_out)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# only keep required columns\u001b[39;00m\n\u001b[1;32m     29\u001b[0m sumstats \u001b[38;5;241m=\u001b[39m sumstats\u001b[38;5;241m.\u001b[39mkey_by(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocus\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m---> 30\u001b[0m sumstats \u001b[38;5;241m=\u001b[39m \u001b[43msumstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdbSNP_rsid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malleles2_sumstats_fixstrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malleles1_sumstats_fixstrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeta_meta_hq_fix_ref_alt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mneglog10_pval_meta_hq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m sumstats\u001b[38;5;241m.\u001b[39mexport(filename_out)\n",
      "File \u001b[0;32m<decorator-gen-1224>:2\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/typecheck/check.py:585\u001b[0m, in \u001b[0;36m_make_dec.<locals>.wrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;129m@decorator\u001b[39m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(__original_func: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    584\u001b[0m     args_, kwargs_ \u001b[38;5;241m=\u001b[39m check_all(__original_func, args, kwargs, checkers, is_method\u001b[38;5;241m=\u001b[39mis_method)\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__original_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:1648\u001b[0m, in \u001b[0;36mTable.select\u001b[0;34m(self, *exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[38;5;129m@typecheck_method\u001b[39m(exprs\u001b[38;5;241m=\u001b[39moneof(Expression, \u001b[38;5;28mstr\u001b[39m), named_exprs\u001b[38;5;241m=\u001b[39manytype)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexprs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Select existing fields or create new fields by name, dropping the rest.\u001b[39;00m\n\u001b[1;32m   1566\u001b[0m \n\u001b[1;32m   1567\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1646\u001b[0m \u001b[38;5;124;03m        Table with specified fields.\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43mget_select_exprs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTable.select\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_row_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_row\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable.select\u001b[39m\u001b[38;5;124m'\u001b[39m, row)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/utils/misc.py:468\u001b[0m, in \u001b[0;36mget_select_exprs\u001b[0;34m(caller, exprs, named_exprs, indices, base_struct)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_select_exprs\u001b[39m(caller, exprs, named_exprs, indices, base_struct):\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpressions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpressionException, analyze, to_expr\n\u001b[0;32m--> 468\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m [indices\u001b[38;5;241m.\u001b[39msource[e] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m exprs]\n\u001b[1;32m    469\u001b[0m     named_exprs \u001b[38;5;241m=\u001b[39m {k: to_expr(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m named_exprs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    470\u001b[0m     select_fields \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mprotected_key[:]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/utils/misc.py:468\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_select_exprs\u001b[39m(caller, exprs, named_exprs, indices, base_struct):\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpressions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpressionException, analyze, to_expr\n\u001b[0;32m--> 468\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m [\u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m[\u001b[49m\u001b[43me\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m e \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m exprs]\n\u001b[1;32m    469\u001b[0m     named_exprs \u001b[38;5;241m=\u001b[39m {k: to_expr(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m named_exprs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    470\u001b[0m     select_fields \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mprotected_key[:]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:419\u001b[0m, in \u001b[0;36mTable.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 419\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;241m*\u001b[39mwrap_to_tuple(item))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/hail/table.py:152\u001b[0m, in \u001b[0;36mExprContainer._get_field\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fields:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fields[item]\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(get_nice_field_error(\u001b[38;5;28mself\u001b[39m, item))\n",
      "\u001b[0;31mLookupError\u001b[0m: Table instance has no field 'beta_meta_hq_fix_ref_alt'\n    Did you mean:\n        'beta_meta_fix_ref_alt' [row]\n    Hint: use 'describe()' to show the names of all data fields."
     ]
    }
   ],
   "source": [
    "print(\"Height \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_Height_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_Height_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"DBP \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_DBP_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_DBP_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"HDL \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_HDL_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_HDL_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"TC \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_TC_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_TC_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"RBC \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_RBC_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_RBC_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"leukocyte \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_leukocyte_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_leukocyte_QCed_dbsnp.tsv.bgz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sumstats_QC(filename_in, filename_out):\n",
    "    sumstats = hl.import_table(filename_in, \n",
    "                               #impute=True, \n",
    "                               types = {\"locus\": hl.tlocus(\"GRCh38\")})\n",
    "    # Add a new column \"alleles\" by combining \"alleles1_sumstats_fixstrand\" and \"alleles2_sumstats_fixstrand\"\n",
    "    sumstats = sumstats.annotate(\n",
    "    alleles=[sumstats.alleles1_sumstats_fixstrand, sumstats.alleles2_sumstats_fixstrand]\n",
    "    )\n",
    "    # Set the type of \"alleles\" explicitly to array<str> (Hail infers it automatically in this case)\n",
    "    sumstats = sumstats.annotate(\n",
    "    alleles=hl.array(sumstats.alleles)\n",
    "    )\n",
    "\n",
    "    sumstats = sumstats.key_by('locus', 'alleles') #for annotate dbSNP\n",
    "    sumstats = sumstats.repartition(100)\n",
    "    \n",
    "    # annotate dbSNP rsID \n",
    "    db = hl.experimental.DB(region='us-central1', cloud='gcp')\n",
    "    sumstats = db.annotate_rows_db(sumstats, 'dbSNP_rsid')\n",
    "    \n",
    "    # Simplify the format of wgs<struct{rsid: str}> to a delimited string\n",
    "    sumstats = sumstats.annotate(\n",
    "    dbSNP_rsid=hl.delimit(sumstats.dbSNP_rsid.map(lambda x: x.rsid), delimiter=\",\")\n",
    "    )\n",
    "    \n",
    "    # only keep required columns\n",
    "    sumstats = sumstats.key_by('locus') \n",
    "    sumstats = sumstats.select(\n",
    "    'dbSNP_rsid',   \n",
    "    'alleles2_sumstats_fixstrand',\n",
    "    'alleles1_sumstats_fixstrand',\n",
    "    'beta_meta_fix_ref_alt',\n",
    "    'neglog10_pval_meta'\n",
    "    )\n",
    "    \n",
    "    sumstats.export(filename_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2D 2024-11-28 02:02:54.577540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 02:02:55.348 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-11-28 02:03:01.748 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 02:04:21.191 Hail: INFO: merging 100 files totalling 15.1M...) / 100]\n",
      "2024-11-28 02:04:21.781 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_T2D_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 589.915ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asthma 2024-11-28 02:04:21.785529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 02:04:22.841 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-11-28 02:04:29.368 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 02:05:39.059 Hail: INFO: merging 100 files totalling 15.2M...) / 100]\n",
      "2024-11-28 02:05:39.704 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_Asthma_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 644.922ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast_Cancer 2024-11-28 02:05:39.708792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 02:05:40.683 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-11-28 02:05:47.136 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 02:06:57.260 Hail: INFO: merging 100 files totalling 15.1M...) / 100]\n",
      "2024-11-28 02:06:57.947 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_Breast_Cancer_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 687.526ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorectal_Cancer 2024-11-28 02:06:57.952122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 02:06:58.949 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_array' as type str (not specified)\n",
      "  Loading field 'alleles2_array' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-11-28 02:07:05.586 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-11-28 02:08:17.482 Hail: INFO: merging 100 files totalling 15.1M...) / 100]\n",
      "2024-11-28 02:08:18.033 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/Array_Colorectal_Cancer_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 550.767ms\n"
     ]
    }
   ],
   "source": [
    "print(\"T2D \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_T2D_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_T2D_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"Asthma \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_Asthma_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_Asthma_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"Breast_Cancer \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_Breast_Cancer_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_Breast_Cancer_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"Colorectal_Cancer \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/Array_Colorectal_Cancer_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/Array_Colorectal_Cancer_QCed_dbsnp.tsv.bgz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGS QC'd sumstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumstats_QC(filename_in, filename_out):\n",
    "    sumstats = hl.import_table(filename_in, \n",
    "                               #impute=True, \n",
    "                               types = {\"locus\": hl.tlocus(\"GRCh38\")})\n",
    "    # Add a new column \"alleles\" by combining \"alleles1_sumstats_fixstrand\" and \"alleles2_sumstats_fixstrand\"\n",
    "    sumstats = sumstats.annotate(\n",
    "    alleles=[sumstats.alleles1_sumstats_fixstrand, sumstats.alleles2_sumstats_fixstrand]\n",
    "    )\n",
    "    # Set the type of \"alleles\" explicitly to array<str> (Hail infers it automatically in this case)\n",
    "    sumstats = sumstats.annotate(\n",
    "    alleles=hl.array(sumstats.alleles)\n",
    "    )\n",
    "\n",
    "    sumstats = sumstats.key_by('locus', 'alleles') #for annotate dbSNP\n",
    "    sumstats = sumstats.repartition(1000)\n",
    "    \n",
    "    # annotate dbSNP rsID \n",
    "    db = hl.experimental.DB(region='us-central1', cloud='gcp')\n",
    "    sumstats = db.annotate_rows_db(sumstats, 'dbSNP_rsid')\n",
    "    \n",
    "    # Simplify the format of wgs<struct{rsid: str}> to a delimited string\n",
    "    sumstats = sumstats.annotate(\n",
    "    dbSNP_rsid=hl.delimit(sumstats.dbSNP_rsid.map(lambda x: x.rsid), delimiter=\",\")\n",
    "    )\n",
    "    \n",
    "    # only keep required columns\n",
    "    sumstats = sumstats.key_by('locus') \n",
    "    sumstats = sumstats.select(\n",
    "    'dbSNP_rsid',   \n",
    "    'alleles2_sumstats_fixstrand',\n",
    "    'alleles1_sumstats_fixstrand',\n",
    "    'beta_meta_fix_ref_alt',\n",
    "    'neglog10_pval_meta'\n",
    "    )\n",
    "    \n",
    "    sumstats.export(filename_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Hail with default parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height 2024-12-17 03:22:12.602822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:43: UserWarning:\n",
      "\n",
      "Reading spark-defaults.conf to determine GCS requester pays configuration. This is deprecated. Please use `hailctl config set gcs_requester_pays/project` and `hailctl config set gcs_requester_pays/buckets`.\n",
      "\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Running on Apache Spark version 3.3.0\n",
      "SparkUI available at http://all-of-us-11150-m.us-central1-a.c.terra-vpc-sc-fd39b54c.internal:46487\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.130.post1-c69cd67afb8b\n",
      "LOGGING: writing to /home/jupyter/workspaces/prswithwgsvsarraydata/hail-20241217-0322-0.2.130.post1-c69cd67afb8b.log\n",
      "2024-12-17 03:22:55.453 Hail: INFO: Reading table without type imputation1) / 1]\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:23:39.737 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 03:30:35.235 Hail: INFO: merging 1000 files totalling 135.6M... 1000]\n",
      "2024-12-17 03:30:40.457 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_Height_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 5.221s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBP 2024-12-17 03:30:40.470220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 03:30:42.438 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:31:25.984 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 03:35:34.239 Hail: INFO: merging 1000 files totalling 134.1M... 1000]\n",
      "2024-12-17 03:35:38.584 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_DBP_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 4.345s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDL 2024-12-17 03:35:38.588868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 03:35:39.883 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:36:23.761 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 03:40:36.058 Hail: INFO: merging 1000 files totalling 133.9M... 1000]\n",
      "2024-12-17 03:40:40.239 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_HDL_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 4.181s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TC 2024-12-17 03:40:40.244575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 03:40:42.037 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:41:26.922 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 03:45:41.189 Hail: INFO: merging 1000 files totalling 134.2M... 1000]\n",
      "2024-12-17 03:45:45.145 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_TC_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 3.956s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBC 2024-12-17 03:45:45.149549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 03:45:46.567 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:46:30.526 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 03:50:45.455 Hail: INFO: merging 1000 files totalling 134.7M... 1000]\n",
      "2024-12-17 03:50:49.282 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_RBC_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 3.827s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leukocyte 2024-12-17 03:50:49.286805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 03:50:50.724 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:51:34.573 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 03:55:38.860 Hail: INFO: merging 1000 files totalling 134.1M... 1000]\n",
      "2024-12-17 03:55:42.549 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_leukocyte_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 3.688s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2D 2024-12-17 03:55:42.553279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 03:55:43.688 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 03:56:25.177 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 04:00:32.000 Hail: INFO: merging 1000 files totalling 135.5M... 1000]\n",
      "2024-12-17 04:00:36.083 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_T2D_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 4.083s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asthma 2024-12-17 04:00:36.087674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 04:00:37.375 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 04:01:19.850 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 04:05:24.098 Hail: INFO: merging 1000 files totalling 135.6M... 1000]\n",
      "2024-12-17 04:05:27.694 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_Asthma_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 3.596s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast_Cancer 2024-12-17 04:05:27.698792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 04:05:29.027 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 04:06:15.218 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 04:10:18.685 Hail: INFO: merging 1000 files totalling 135.4M... 1000]\n",
      "2024-12-17 04:10:22.452 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_Breast_Cancer_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 3.766s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorectal_Cancer 2024-12-17 04:10:22.456707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 04:10:23.638 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'locus' as type locus<GRCh38> (user-supplied)\n",
      "  Loading field 'rsid' as type str (not specified)\n",
      "  Loading field 'alleles1_wgs' as type str (not specified)\n",
      "  Loading field 'alleles2_wgs' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_original' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_original' as type str (not specified)\n",
      "  Loading field 'is_negative_strand' as type str (not specified)\n",
      "  Loading field 'alleles1_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'alleles2_sumstats_fixstrand' as type str (not specified)\n",
      "  Loading field 'beta_meta' as type str (not specified)\n",
      "  Loading field 'beta_meta_fix_ref_alt' as type str (not specified)\n",
      "  Loading field 'se_meta' as type str (not specified)\n",
      "  Loading field 'neglog10_pval_meta' as type str (not specified)\n",
      "2024-12-17 04:11:09.153 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2024-12-17 04:15:12.511 Hail: INFO: merging 1000 files totalling 134.2M... 1000]\n",
      "2024-12-17 04:15:16.303 Hail: INFO: while writing:\n",
      "    gs://fc-secure-9afe7562-2fad-4781-ab60-03528a626c19/Sumstats/WGS_Colorectal_Cancer_QCed_dbsnp.tsv.bgz\n",
      "  merge time: 3.792s\n"
     ]
    }
   ],
   "source": [
    "print(\"Height \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_Height_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_Height_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"DBP \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_DBP_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_DBP_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"HDL \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_HDL_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_HDL_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"TC \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_TC_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_TC_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"RBC \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_RBC_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_RBC_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"leukocyte \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_leukocyte_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_leukocyte_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"T2D \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_T2D_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_T2D_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"Asthma \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_Asthma_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_Asthma_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"Breast_Cancer \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_Breast_Cancer_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_Breast_Cancer_QCed_dbsnp.tsv.bgz\")\n",
    "\n",
    "print(\"Colorectal_Cancer \" + str(datetime.now()))\n",
    "sumstats_QC(f\"{bucket}/Sumstats/WGS_Colorectal_Cancer_QCed.tsv.bgz\",\n",
    "            f\"{bucket}/Sumstats/WGS_Colorectal_Cancer_QCed_dbsnp.tsv.bgz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to file size limitation for download, save the files with locus or rsID separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgs_Height = pd.read_csv(\"WGS_Height_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_DBP = pd.read_csv(\"WGS_DBP_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_HDL = pd.read_csv(\"WGS_HDL_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_TC = pd.read_csv(\"WGS_TC_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_RBC = pd.read_csv(\"WGS_RBC_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_leukocyte = pd.read_csv(\"WGS_leukocyte_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "\n",
    "wgs_Asthma = pd.read_csv(\"WGS_Asthma_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_T2D = pd.read_csv(\"WGS_T2D_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_Colorectal_Cancer = pd.read_csv(\"WGS_Colorectal_Cancer_QCed_dbsnp.tsv.gz\",sep = \"\\t\")\n",
    "wgs_Breast_Cancer = pd.read_csv(\"WGS_Breast_Cancer_QCed_dbsnp.tsv.gz\",sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the second column (index 1)\n",
    "Height = wgs_Height.drop(wgs_Height.columns[1], axis=1)\n",
    "DBP = wgs_DBP.drop(wgs_DBP.columns[1], axis=1)\n",
    "HDL = wgs_HDL.drop(wgs_HDL.columns[1], axis=1)\n",
    "TC = wgs_TC.drop(wgs_TC.columns[1], axis=1)\n",
    "RBC = wgs_RBC.drop(wgs_RBC.columns[1], axis=1)\n",
    "leukocyte = wgs_leukocyte.drop(wgs_leukocyte.columns[1], axis=1)\n",
    "Asthma = wgs_Asthma.drop(wgs_Asthma.columns[1], axis=1)\n",
    "T2D = wgs_T2D.drop(wgs_T2D.columns[1], axis=1)\n",
    "Colorectal_Cancer = wgs_Colorectal_Cancer.drop(wgs_Colorectal_Cancer.columns[1], axis=1)\n",
    "Breast_Cancer = wgs_Breast_Cancer.drop(wgs_Breast_Cancer.columns[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of phenotypes\n",
    "phenotypes = ['Height', 'DBP', 'HDL', 'TC', 'RBC', 'leukocyte', 'Asthma', 'T2D', 'Colorectal_Cancer', 'Breast_Cancer']\n",
    "\n",
    "# Loop through each phenotype and save the corresponding dataframe to a file\n",
    "for phenotype in phenotypes:\n",
    "    globals()[phenotype].to_csv(f'WGS_{phenotype}_QCed.tsv.gz', sep=\"\\t\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
